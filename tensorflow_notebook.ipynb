{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 40\n",
    "NUM_BATCHES_TRAIN = 450\n",
    "NUM_BATCHES_TEST = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_csv = open('train.csv','a')\n",
    "train_writer = csv.writer(train_csv,delimiter=',')\n",
    "test_csv = open('test.csv','a')\n",
    "test_writer = csv.writer(test_csv,delimiter=',')\n",
    "with open('fer2013.csv',\"rt\") as csvfile:\n",
    "    reader = csv.reader(csvfile,delimiter = ',')\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        if i == 0:\n",
    "            i+= 1\n",
    "            continue\n",
    "        if row[2] == 'Training':\n",
    "            train_writer.writerow(row)\n",
    "        else:\n",
    "            test_writer.writerow(row)\n",
    "        i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_csv.close()\n",
    "test_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to generate batches of data\n",
    "def generate_sample(filename):\n",
    "    while True:\n",
    "        with open(filename,'rt') as csvfile:\n",
    "            reader = csv.reader(csvfile,delimiter = ',')\n",
    "            for row in reader:\n",
    "                y = np.zeros(7)\n",
    "                y[row[0]] = 1\n",
    "                yield (np.array(row[1].split()).astype(np.float),y)\n",
    "                       \n",
    "def get_batch(filename,batch_size = BATCH_SIZE):\n",
    "    itr = generate_sample(filename)\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for i in range(batch_size):\n",
    "            a = next(itr)\n",
    "            batch_x.append(a[0])\n",
    "            batch_y.append(a[1])\n",
    "        yield np.array(batch_x),np.array(batch_y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function in case you want to save images in folders. Not required though for our purpose\n",
    "def save_images(filename,mode):\n",
    "    with open(filename,'rt') as csvfile:\n",
    "        reader = csv.reader(csvfile,delimiter = ',')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            img = np.array(row[1].split()).astype(np.float)\n",
    "            img = img.reshape([48,48,1])\n",
    "            img = np.concatenate((img,img,img),axis = 2)\n",
    "            img = img.astype(np.uint8)\n",
    "            img = Image.fromarray(img)\n",
    "            img.save('data/'+mode+'/'+str(row[0]) + '/'+str(i)+'.jpg')\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_images('train.csv','train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_images('test.csv','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = get_batch('train.csv',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = c[0][0].reshape(48,48)\n",
    "print(img.shape)\n",
    "img = Image.fromarray(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.concatenate((a,a,a),axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = a.reshape([48,48,3])\n",
    "a = a.astype(np.uint8)\n",
    "img = Image.fromarray(a)\n",
    "print(img.size)\n",
    "img.save('foo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = Image.open('foo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.array(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholders\n",
    "input_placeholder = tf.placeholder(tf.float32, shape = [BATCH_SIZE,2304])\n",
    "output_placeholder = tf.placeholder(tf.float32, shape = [BATCH_SIZE, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building CNN\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=0.25)\n",
    "a1 = tf.reshape(input_placeholder,[-1,48,48,1])\n",
    "conv1 = tf.layers.conv2d(\n",
    "        inputs = a1,\n",
    "        filters = 64,\n",
    "        kernel_size = 3,\n",
    "        padding = 'same',\n",
    "        kernel_regularizer = regularizer,\n",
    "        activation = tf.nn.relu)\n",
    "\n",
    "bn1 = tf.layers.batch_normalization(conv1,axis = 1)\n",
    "conv2 = tf.layers.conv2d(\n",
    "        inputs = bn1,\n",
    "        filters = 64,\n",
    "        kernel_size = 3,\n",
    "        padding = 'same',\n",
    "        kernel_regularizer = regularizer,\n",
    "        activation = tf.nn.relu)\n",
    "bn2 = tf.layers.batch_normalization(conv2, axis = 1)\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "        inputs = bn2,\n",
    "        pool_size = [2,2],\n",
    "        strides = 2)\n",
    "\n",
    "conv3 = tf.layers.conv2d(\n",
    "        inputs = pool1,\n",
    "        filters = 128,\n",
    "        kernel_size = 3,\n",
    "        padding = 'same',\n",
    "        kernel_regularizer = regularizer,\n",
    "        activation = tf.nn.relu)\n",
    "\n",
    "bn3 = tf.layers.batch_normalization(conv3, axis = 1)\n",
    "\n",
    "conv4 = tf.layers.conv2d(\n",
    "        inputs = bn3,\n",
    "        filters = 128,\n",
    "        kernel_size = 3,\n",
    "        padding = 'same',\n",
    "        kernel_regularizer = regularizer,\n",
    "        activation = tf.nn.relu)\n",
    "\n",
    "bn4 = tf.layers.batch_normalization(conv4, axis = 1)\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "        inputs = bn4,\n",
    "        pool_size = [2,2],\n",
    "        strides = 2)\n",
    "\n",
    "avg_pool = tf.nn.avg_pool(pool2,[1,12,12,1],strides=[1,1,1,1],padding=\"VALID\")\n",
    "avg_pool = tf.reshape(avg_pool,[-1,128])\n",
    "dense = tf.layers.dense(inputs=avg_pool,units=1024,kernel_regularizer = regularizer,activation=tf.nn.relu)\n",
    "bn5 = tf.layers.batch_normalization(dense,axis=1)\n",
    "y_out = tf.layers.dense(inputs = bn5, units=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating Cross Entropy Loss\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels = output_placeholder, logits = y_out)\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating Adam Optimizer\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_step = optimizer.minimize(loss,global_step)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    itr = get_batch('train.csv')\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    initial_step = global_step.eval()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for i in range(initial_step,NUM_EPOCHS*NUM_BATCHES_TRAIN):\n",
    "        X_batch, y_batch = next(itr)\n",
    "        _,loss_batch = sess.run([train_step,loss],feed_dict={\n",
    "            input_placeholder : X_batch,\n",
    "            output_placeholder : y_batch\n",
    "        })\n",
    "        total_loss += loss_batch\n",
    "        if i%200 == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(i + 1, total_loss / 200))\n",
    "            saver.save(sess, 'checkpoints/fer_convenet', i)\n",
    "            total_loss = 0\n",
    "            itrtest = get_batch('test.csv')\n",
    "            total_correct_preds = 0\n",
    "            for i in range(20):\n",
    "                X_batch, y_batch = next(itrtest)\n",
    "                pred = sess.run(y_out,feed_dict = {\n",
    "                    input_placeholder : X_batch,\n",
    "                    output_placeholder : y_batch\n",
    "                })\n",
    "                pred = tf.nn.softmax(pred)\n",
    "                correct_preds = tf.equal(tf.argmax(pred),tf.argmax(y_batch))\n",
    "                accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32))\n",
    "                total_correct_preds += sess.run(accuracy)\n",
    "            print(\"Accuracy {0}\".format(total_correct_preds/(20*BATCH_SIZE)))\n",
    "            \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    itr = get_batch('test.csv')\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    for i in range(NUM_BATCHES_TEST):\n",
    "        X_batch, y_batch = next(itr)\n",
    "        pred = sess.run(y_out,feed_dict = {\n",
    "            input_placeholder : X_batch,\n",
    "            output_placeholder : y_batch\n",
    "        })\n",
    "        pred = tf.nn.softmax(pred)\n",
    "        correct_preds = tf.equal(tf.argmax(pred,1),tf.argmax(y_batch,1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/(NUM_BATCHES_TEST*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.get_default_graph().as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    itr = get_batch('test.csv')\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(NUM_BATCHES_TEST):\n",
    "        X_batch, y_batch = next(itr)\n",
    "        pred = sess.run(y_out,feed_dict = {\n",
    "            input_placeholder : X_batch,\n",
    "            output_placeholder : y_batch\n",
    "        })\n",
    "        pred = tf.nn.softmax(pred)\n",
    "        correct_preds = tf.equal(tf.argmax(pred,1),tf.argmax(y_batch,1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/(NUM_BATCHES_TEST*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
